{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNtZbRtXMDk0DTnJ7RPV5Zj"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kSBKDqBanMBT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "#physical_devices = tf.config.list_physical_devices('GPU')\n",
        "#tf.config.experimental.set_memory_growth(physical_devices[0], True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')"
      ],
      "metadata": {
        "id": "cOA2CJIfnvjp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sequntial API\n",
        "#We are going to use the Conv2D method and Maxpool2D from layers\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(32, 32, 3)),\n",
        "        layers.Conv2D(32, 3, padding = 'valid', activation = 'relu' ),\n",
        "        layers.MaxPool2D(),\n",
        "        layers.Conv2D(64, 3, activation = 'relu'),\n",
        "        layers.MaxPool2D(),\n",
        "        layers.Conv2D(128, 3, activation = 'relu'),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(10)\n",
        "\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
        "    optimizer = keras.optimizers.Adam(learning_rate = 3e-4),\n",
        "    metrics = ['accuracy']\n",
        ")\n",
        "\n",
        "\n",
        "model.fit(x_train, y_train, batch_size =64, epochs = 10, verbose =2)\n",
        "model.evaluate(x_test, y_test, batch_size = 64, verbose = 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgYBZweZoEfh",
        "outputId": "20b4d17a-0402-40a8-caa9-36b24e39eb94"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "782/782 - 9s - 11ms/step - accuracy: 0.3084 - loss: 2.0938\n",
            "Epoch 2/10\n",
            "782/782 - 2s - 3ms/step - accuracy: 0.4693 - loss: 1.4788\n",
            "Epoch 3/10\n",
            "782/782 - 3s - 3ms/step - accuracy: 0.5510 - loss: 1.2662\n",
            "Epoch 4/10\n",
            "782/782 - 3s - 4ms/step - accuracy: 0.6096 - loss: 1.1116\n",
            "Epoch 5/10\n",
            "782/782 - 2s - 3ms/step - accuracy: 0.6550 - loss: 0.9923\n",
            "Epoch 6/10\n",
            "782/782 - 3s - 3ms/step - accuracy: 0.6881 - loss: 0.8989\n",
            "Epoch 7/10\n",
            "782/782 - 2s - 3ms/step - accuracy: 0.7209 - loss: 0.8098\n",
            "Epoch 8/10\n",
            "782/782 - 2s - 3ms/step - accuracy: 0.7468 - loss: 0.7293\n",
            "Epoch 9/10\n",
            "782/782 - 3s - 4ms/step - accuracy: 0.7711 - loss: 0.6583\n",
            "Epoch 10/10\n",
            "782/782 - 2s - 3ms/step - accuracy: 0.7910 - loss: 0.6005\n",
            "157/157 - 1s - 8ms/step - accuracy: 0.6813 - loss: 0.9949\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9949035048484802, 0.6812999844551086]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Functional API\n",
        "# We are going to use Batch Normalization before we feed our convolutoin layer to the activation layer\n",
        "#Batch norm is a statistical technique\n",
        "\n",
        "def f_model():\n",
        "  inputs = keras.Input(shape =(32, 32, 3))\n",
        "  x = layers.Conv2D(32, 3)(inputs)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.ReLU()(x)\n",
        "  x = layers.MaxPool2D()(x)\n",
        "  x = layers.Conv2D(64, 5, padding = 'same')(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.ReLU()(x)\n",
        "  x = layers.MaxPool2D()(x)\n",
        "  x = layers.Conv2D(128, 3)(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.ReLU()(x)\n",
        "  x = layers.Flatten()(x)\n",
        "  x = layers.Dense(64, activation = 'relu')(x)\n",
        "  outputs = layers.Dense(10)(x)\n",
        "  model = keras.Model(inputs = inputs, outputs = outputs )\n",
        "  return model\n",
        "\n",
        "model = f_model()\n",
        "\n",
        "model.compile(\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
        "    optimizer = keras.optimizers.Adam(learning_rate = 3e-4),\n",
        "    metrics = ['accuracy']\n",
        ")\n",
        "model.fit(x_train, y_train, batch_size = 64, epochs = 10, verbose = 2)\n",
        "model.evaluate(x_test, y_test, batch_size = 64, verbose = 2)\n",
        "\n",
        "#We notice that our accuracy has improved but still has a long way to go, we can train it for longer\n",
        "#The accuracy is low because this model has only 255k parameters compared to AlexNet which had 60m\n",
        "#The reason our accuracy improved from the sequential API is because we used Batch Normalization\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzOYZc_bpzHp",
        "outputId": "896321e8-87dc-4ead-a33b-b8d1564c6db4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "782/782 - 10s - 13ms/step - accuracy: 0.5423 - loss: 1.2820\n",
            "Epoch 2/10\n",
            "782/782 - 3s - 4ms/step - accuracy: 0.6866 - loss: 0.8932\n",
            "Epoch 3/10\n",
            "782/782 - 3s - 4ms/step - accuracy: 0.7410 - loss: 0.7354\n",
            "Epoch 4/10\n",
            "782/782 - 5s - 6ms/step - accuracy: 0.7803 - loss: 0.6288\n",
            "Epoch 5/10\n",
            "782/782 - 3s - 4ms/step - accuracy: 0.8097 - loss: 0.5479\n",
            "Epoch 6/10\n",
            "782/782 - 6s - 8ms/step - accuracy: 0.8359 - loss: 0.4766\n",
            "Epoch 7/10\n",
            "782/782 - 3s - 4ms/step - accuracy: 0.8556 - loss: 0.4150\n",
            "Epoch 8/10\n",
            "782/782 - 6s - 7ms/step - accuracy: 0.8806 - loss: 0.3544\n",
            "Epoch 9/10\n",
            "782/782 - 5s - 6ms/step - accuracy: 0.8973 - loss: 0.3047\n",
            "Epoch 10/10\n",
            "782/782 - 3s - 4ms/step - accuracy: 0.9142 - loss: 0.2551\n",
            "157/157 - 2s - 12ms/step - accuracy: 0.7050 - loss: 1.0363\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0362980365753174, 0.7049999833106995]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jmEwoX3itidJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}